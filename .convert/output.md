* **MST: Masked Self-Supervised Transformer for Visual Representation**<br>
*Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong Zhu, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, Ming Tang, Jinqiao Wang*<br>
NIPS'2021 [[Paper](https://arxiv.org/abs/2106.05656)]
   <details close>
   <summary>MST Framework</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/44519745/204311330-9652d5d0-4b94-4f9a-afcd-efc12c712279.png" /></p>
   </details>

* **Are Large-scale Datasets Necessary for Self-Supervised Pre-training**<br>
*Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv√© Jegou, Edouard Grave*<br>
ArXiv'2021 [[Paper](https://arxiv.org/abs/2112.10740)]
   <details close>
   <summary>SplitMask Framework</summary>
   <p align="center"><img width="60%" src="https://user-images.githubusercontent.com/44519745/204311839-6f1310c9-88b2-4f43-90ff-927cf8aba720.png" /></p>
   </details>

* **Masked Siamese Networks for Label-Efficient Learning**<br>
*Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas*<br>
ArXiv'2022 [[Paper](https://arxiv.org/abs/2204.07141)]
[[Code](https://github.com/facebookresearch/msn)]
   <details close>
   <summary>MSN Framework</summary>
   <p align="center"><img width="85%" src="https://user-images.githubusercontent.com/44519745/204312102-a35d65ac-61e6-46ba-bb86-6c18b8562966.png" /></p>
   </details>

* **Siamese Image Modeling for Self-Supervised Vision Representation Learning**<br>
*Chenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, Jifeng Dai*<br>
ArXiv'2022 [[Paper](https://arxiv.org/abs/2206.01204)]
[[Code](https://github.com/fundamentalvision/Siamese-Image-Modeling)]
   <details close>
   <summary>SIM Framework</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/44519745/204312408-fe573880-62ac-4f6e-b7ed-c9163f0cea96.png" /></p>
   </details>

* **Masked Image Modeling with Denoising Contrast**<br>
*Kun Yi, Yixiao Ge, Xiaotong Li, Shusheng Yang, Dian Li, Jianping Wu, Ying Shan, Xiaohu Qie*<br>
ICLR'2023 [[Paper](https://arxiv.org/abs/2205.09616)]
[[Code](https://github.com/TencentARC/ConMIM)]
   <details close>
   <summary>ConMIM Framework</summary>
   <p align="center"><img width="80%" src="https://user-images.githubusercontent.com/44519745/204312585-13d5094b-c90c-4ab6-88d1-b88d46d8ae62.png" /></p>
   </details>

* **RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training**<br>
*Luya Wang, Feng Liang, Yangguang Li, Honggang Zhang, Wanli Ouyang, Jing Shao*<br>
ArXiv'2022 [[Paper](https://arxiv.org/abs/2201.06857)]
   <details close>
   <summary>RePre Framework</summary>
   <p align="center"><img width="95%" src="https://user-images.githubusercontent.com/44519745/204312825-03953a52-0c1a-4f7e-bf12-e13841c2d371.png" /></p>
   </details>

* **Masked Siamese ConvNets**<br>
*Li Jing, Jiachen Zhu, Yann LeCun*<br>
ArXiv'2022 [[Paper](https://arxiv.org/abs/2206.07700)]
   <details close>
   <summary>MSCN Framework</summary>
   <p align="center"><img width="85%" src="https://user-images.githubusercontent.com/44519745/216648027-99790176-87fa-4fc6-ad5f-a8fe255c60e6.png" /></p>
   </details>

* **Contrastive Masked Autoencoders are Stronger Vision Learners**<br>
*Zhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou, Ming-Ming Cheng, Dongmei Fu, Xiaohui Shen, Jiashi Feng*<br>
ArXiv'2022 [[Paper](https://arxiv.org/abs/2207.13532)]
[[Code](https://github.com/ZhichengHuang/CMAE)]
   <details close>
   <summary>CMAE Framework</summary>
   <p align="center"><img width="85%" src="https://user-images.githubusercontent.com/44519745/204313292-54630e16-e8ea-4281-a922-1b08c860e721.png" /></p>
   </details>

* **A simple, efficient and scalable contrastive masked autoencoder for learning visual representations**<br>
*Shlok Mishra, Joshua Robinson, Huiwen Chang, David Jacobs, Aaron Sarna, Aaron Maschinot, Dilip Krishnan*<br>
ArXiv'2022 [[Paper](https://arxiv.org/abs/2210.16870)]
   <details close>
   <summary>CAN Framework</summary>
   <p align="center"><img width="85%" src="https://user-images.githubusercontent.com/44519745/204313772-7c0bf6d4-8df1-4b05-8733-da5024513e10.png" /></p>
   </details>

* **MimCo: Masked Image Modeling Pre-training with Contrastive Teacher**<br>
*Qiang Zhou, Chaohui Yu, Hao Luo, Zhibin Wang, Hao Li*<br>
ArXiv'2022 [[Paper](https://arxiv.org/abs/2209.03063)]
   <details close>
   <summary>MimCo Framework</summary>
   <p align="center"><img width="95%" src="https://user-images.githubusercontent.com/44519745/216651122-8fe6a039-37a8-4bec-8988-2760006da0af.png" /></p>
   </details>

* **Contextual Image Masking Modeling via Synergized Contrasting without View Augmentation for Faster and Better Visual Pretraining**<br>
*Shaofeng Zhang, Feng Zhu, Rui Zhao, Junchi Yan*<br>
ICLR'2023 [[Paper](https://openreview.net/forum?id=A3sgyt4HWp)]
[[Code](https://github.com/Sherrylone/ccMIM)]
   <details close>
   <summary>ccMIM Framework</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/44519745/204314041-63c5e06d-b870-482d-8f6b-e70e1af9d642.png" /></p>
   </details>

* **How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders**<br>
*Qi Zhang, Yifei Wang, Yisen Wang*<br>
NIP'2022 [[Paper](https://arxiv.org/abs/2210.08344)]
[[Code](https://github.com/zhangq327/U-MAE)]
   <details close>
   <summary>UMAE Framework</summary>
   <p align="center"><img width="90%" src="https://user-images.githubusercontent.com/44519745/234359652-b34cb444-1c6b-4721-94e3-6bd60347ca55.png" /></p>
   </details>

* **Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations**<br>
*Ziyu Jiang, Yinpeng Chen, Mengchen Liu, Dongdong Chen, Xiyang Dai, Lu Yuan, Zicheng Liu, Zhangyang Wang*<br>
ICLR'2023 [[Paper](https://openreview.net/forum?id=jwdqNwyREyh)]
[[Code](https://github.com/VITA-Group/layerGraftedPretraining_ICLR23)]
   <details close>
   <summary>LayerGrafted Framework</summary>
   <p align="center"><img width="80%" src="https://user-images.githubusercontent.com/44519745/224830983-13cfcbf5-f1df-481b-9e7c-24667d041fe4.png" /></p>
   </details>

* **Self-supervision through Random Segments with Autoregressive Coding (RandSAC)**<br>
*Tianyu Hua, Yonglong Tian, Sucheng Ren, Michalis Raptis, Hang Zhao, Leonid Sigal*<br>
ICLR'2023 [[Paper](https://arxiv.org/abs/2203.12054)]
   <details close>
   <summary>RandSAC Framework</summary>
   <p align="center"><img width="80%" src="https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/261103618-f2aa7486-a09f-4f50-a84d-fb367c621d04.png" /></p>
   </details>

